{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "df = pd.read_csv('data/2-descriptor/all_descriptors.csv')\n",
    "\n",
    "\n",
    "X = df.drop(['ID', 'label', 'SMILES'], axis=1)\n",
    "\n",
    "\n",
    "X = X.dropna(axis=1, how='all')\n",
    "X = X.fillna(X.median())\n",
    "constant_columns = X.columns[X.std() == 0]\n",
    "X = X.drop(columns=constant_columns)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "         cumulative_variance_ratio, 'bo-')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance ratio')\n",
    "plt.title('PCA Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.savefig('Plots/3-feature/pca_variance_ratio.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"Explain the number of principal components required to account for 95% of the variance: {n_components_95}\")\n",
    "\n",
    "\n",
    "pca_final = PCA(n_components=n_components_95)\n",
    "X_pca_final = pca_final.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "feature_importance = pd.DataFrame(\n",
    "    data=np.abs(pca_final.components_.T),  # 转置，使特征为行\n",
    "    index=X.columns,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components_95)]\n",
    ")\n",
    "\n",
    "top_features_per_pc = {}\n",
    "for i in range(min(5, n_components_95)):  # 展示前5个主成分\n",
    "    pc_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': np.abs(pca_final.components_[i])\n",
    "    })\n",
    "    top_features = pc_importance.nlargest(10, 'importance')\n",
    "    top_features_per_pc[f'PC{i+1}'] = top_features\n",
    "\n",
    "\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca_final, columns=[f'PC{i+1}' for i in range(n_components_95)])\n",
    "pca_df['ID'] = df['ID']\n",
    "pca_df['label'] = df['label']\n",
    "pca_df['SMILES'] = df['SMILES']\n",
    "\n",
    "\n",
    "cols = ['ID', 'SMILES', 'label'] + [col for col in pca_df.columns if col not in ['ID', 'SMILES', 'label']]\n",
    "pca_df = pca_df[cols]\n",
    "\n",
    "\n",
    "pca_df.to_csv('data/3-feature/pca_features.csv', index=False)\n",
    "\n",
    "pca_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca_final.explained_variance_ratio_) + 1),\n",
    "        pca_final.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Components',fontsize=15)\n",
    "plt.ylabel('Percentage of Explained Variance',fontsize=15)\n",
    "plt.title('Percentage of Explained Variance by each Principal Component',fontsize=15)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance_ratio[:n_components_95]) + 1),\n",
    "         cumulative_variance_ratio[:n_components_95], 'bo-')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% cut-off threshold')\n",
    "plt.axvline(x=n_components_95, color='g', linestyle='--', alpha=0.7,\n",
    "           label=f'PC{n_components_95}')\n",
    "plt.xlabel('Number of Components',fontsize=15)\n",
    "plt.ylabel('Cumulative Explained Variance',fontsize=15)\n",
    "plt.title('Cumulative Explained Variance by Number of Components',fontsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/3-feature/pca_variance_ratio.png', dpi=600, bbox_inches='tight')\n",
    "plt.close()"
   ],
   "id": "1c577b59f53ebd15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.makedirs('Predict/feature/pca', exist_ok=True)\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "pca_model_path = 'Predict/feature/pca/pca_model.joblib'\n",
    "joblib.dump(pca_final, pca_model_path)\n",
    "\n",
    "scaler_path = 'Predict/feature/pca/scaler.joblib'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "feature_names_path = 'Predict/feature/pca/feature_names.txt'\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    f.write('\\n'.join(X.columns))\n",
    "\n",
    "pca_info = {\n",
    "    'n_components': int(n_components_95),\n",
    "    'explained_variance_ratio': [float(x) for x in pca_final.explained_variance_ratio_],\n",
    "    'cumulative_variance_ratio': [float(x) for x in cumulative_variance_ratio]\n",
    "}\n",
    "\n",
    "import json\n",
    "pca_info_path = 'Predict/feature/pca/pca_info.json'\n",
    "with open(pca_info_path, 'w') as f:\n",
    "    json.dump(pca_info, f, indent=4)\n"
   ],
   "id": "2f59024168e56dcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/2-descriptor/all_descriptors.csv')\n",
    "\n",
    "X = df.drop(['ID', 'label', 'SMILES'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "X = X.dropna(axis=1, how='all')\n",
    "X = X.fillna(X.median())\n",
    "constant_columns = X.columns[X.std() == 0]\n",
    "X = X.drop(columns=constant_columns)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "k = 20\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "scores = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'score': selector.scores_,\n",
    "    '': selector.pvalues_\n",
    "})\n",
    "\n",
    "scores = scores.sort_values('score', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=scores.head(20), x='score', y='feature')\n",
    "plt.title('Top 20 Features by F-score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/3-feature/kbest_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "selected_features = scores.head(k)['feature'].tolist()\n",
    "X_selected_df = X[selected_features]\n",
    "X_selected_df['ID'] = df['ID']\n",
    "X_selected_df['label'] = df['label']\n",
    "X_selected_df['SMILES'] = df['SMILES']\n",
    "\n",
    "cols = ['ID', 'SMILES', 'label'] + selected_features\n",
    "X_selected_df = X_selected_df[cols]\n",
    "\n",
    "X_selected_df.to_csv('data/3-feature/kbest_features.csv', index=False)\n",
    "\n",
    "\n",
    "X_selected_df"
   ],
   "id": "b6049640edde8af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "\n",
    "os.makedirs('Predict/feature/Kbest', exist_ok=True)\n",
    "\n",
    "\n",
    "kbest_model_path = 'Predict/feature/Kbest/kbest_model.joblib'\n",
    "joblib.dump(selector, kbest_model_path)\n",
    "\n",
    "\n",
    "scaler_path = 'Predict/feature/Kbest/scaler.joblib'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "\n",
    "feature_info = {\n",
    "    'original_features': X.columns.tolist(),\n",
    "    'selected_features': selected_features,\n",
    "    'feature_scores': {\n",
    "        'features': scores['feature'].tolist(),\n",
    "        'scores': [float(x) for x in scores['score']],\n",
    "        'p_values': [float(x) for x in scores['p']]\n",
    "    },\n",
    "    'k_value': k\n",
    "}\n",
    "\n",
    "feature_info_path = 'Predict/feature/Kbest/feature_info.json'\n",
    "with open(feature_info_path, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=4)\n"
   ],
   "id": "f2b8ab8dfa6b4f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier  # 改为随机森林\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/2-descriptor/all_descriptors.csv')\n",
    "\n",
    "\n",
    "X = df.drop(['ID', 'label', 'SMILES'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "X = X.dropna(axis=1, how='all')\n",
    "X = X.fillna(X.median())\n",
    "constant_columns = X.columns[X.std() == 0]\n",
    "X = X.drop(columns=constant_columns)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "\n",
    "n_features_to_select = 100\n",
    "estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=0.1)\n",
    "\n",
    "\n",
    "selector = selector.fit(X_scaled, y)\n",
    "\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'rank': selector.ranking_,\n",
    "    'select': selector.support_\n",
    "})\n",
    "\n",
    "\n",
    "feature_ranking = feature_ranking.sort_values('rank')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "selected_features = feature_ranking[feature_ranking['select']].head(20)\n",
    "plt.bar(range(len(selected_features)), [1]*len(selected_features))\n",
    "plt.xticks(range(len(selected_features)), selected_features['feature'], rotation=45, ha='right')\n",
    "plt.title('Top 20 Selected Features by RFE')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/3-feature/rfe_features.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "selected_features_all = feature_ranking[feature_ranking['select']]['feature'].tolist()\n",
    "X_selected_df = X[selected_features_all]\n",
    "X_selected_df['ID'] = df['ID']\n",
    "X_selected_df['label'] = df['label']\n",
    "X_selected_df['SMILES'] = df['SMILES']\n",
    "\n",
    "cols = ['ID', 'SMILES', 'label'] + selected_features_all\n",
    "X_selected_df = X_selected_df[cols]\n",
    "\n",
    "X_selected_df.to_csv('data/3-feature/rfe_features.csv', index=False)\n",
    "\n",
    "X_selected_df"
   ],
   "id": "e991188a79209bea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "os.makedirs('Predict/feature/RFE', exist_ok=True)\n",
    "\n",
    "rfe_model_path = 'Predict/feature/RFE/rfe_model.joblib'\n",
    "joblib.dump(selector, rfe_model_path)\n",
    "\n",
    "scaler_path = 'Predict/feature/RFE/scaler.joblib'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "feature_info = {\n",
    "    'original_features': X.columns.tolist(),\n",
    "    'selected_features': selected_features['feature'].tolist(),\n",
    "    'feature_ranking': {\n",
    "        'features': feature_ranking['feature'].tolist(),\n",
    "        'rankings': [int(x) for x in feature_ranking['rank']],\n",
    "        'is_selected': feature_ranking['select'].tolist()\n",
    "    },\n",
    "    'n_features_selected': n_features_to_select\n",
    "}\n",
    "\n",
    "feature_info_path = 'Predict/feature/RFE/feature_info.json'\n",
    "with open(feature_info_path, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=4)\n"
   ],
   "id": "e54df84d2ebcd6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"data/2-descriptor/all_descriptors.csv\")\n",
    "\n",
    "\n",
    "features = df.drop(['ID', 'label', 'SMILES'], axis=1) if 'SMILES' in df.columns else df.drop(['label'], axis=1)\n",
    "\n",
    "print(\"\\nColumns with non-numeric data types:\")\n",
    "non_numeric_cols = features.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(non_numeric_cols.tolist())\n",
    "    print(\"\\nSample values from non-numeric columns:\")\n",
    "    for col in non_numeric_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(features[col].value_counts().head())\n",
    "\n",
    "\n",
    "    features = features.select_dtypes(include=['int64', 'float64'])\n",
    "    print(f\"\\nRemoved {len(non_numeric_cols)} non-numeric columns\")\n",
    "else:\n",
    "    print(\"All columns are numeric\")\n",
    "\n",
    "print(f\"\\nFinal shape: {features.shape}\")\n",
    "\n",
    "\n",
    "print(f\"\\nHandling missing values...\")\n",
    "print(f\"Original shape: {features.shape}\")\n",
    "\n",
    "\n",
    "features = features.dropna(axis=1, how='all')\n",
    "print(f\"Shape after dropping all-NaN columns: {features.shape}\")\n",
    "\n",
    "\n",
    "features = features.fillna(features.median())\n",
    "print(f\"Shape after filling NaN values: {features.shape}\")\n",
    "\n",
    "\n",
    "if features.isna().any().any():\n",
    "    print(\"Warning: There are still NaN values in the dataset!\")\n",
    "else:\n",
    "    print(\"All NaN values have been handled successfully.\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "\n",
    "df_processed = pd.DataFrame()\n",
    "df_processed['ID'] = df['ID']\n",
    "df_processed['SMILES'] = df['SMILES']\n",
    "df_processed['label'] = df['label']\n",
    "for col in features_scaled.columns:\n",
    "    df_processed[col] = features_scaled[col]\n",
    "\n",
    "\n",
    "output_path = f'data/3-feature/nofs_features.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"\\nProcessed data saved to: {output_path}\")"
   ],
   "id": "2baa6cc6f180a13e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir('data/3-feature') if f.endswith('.csv')]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(f'data/3-feature/{csv_file}')\n",
    "\n",
    "    df = df.drop(['ID', 'SMILES'], axis=1)\n",
    "\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    sns.countplot(data=df, x='label', hue='label', palette='Set2', legend=False)\n",
    "    plt.title(f'Original Distribution\\n({csv_file})')\n",
    "\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "    df_resampled = pd.DataFrame(\n",
    "        np.column_stack([X_resampled, y_resampled]),\n",
    "        columns=X.columns.tolist() + ['label']\n",
    "    )\n",
    "\n",
    "\n",
    "    df_resampled['label'] = df_resampled['label'].astype(int)\n",
    "\n",
    "\n",
    "    output_filename = f'data/4-upsample/upsampled_{csv_file}'\n",
    "    df_resampled.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(df_resampled['label'].value_counts())\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(data=df_resampled, x='label', hue='label', palette='Set2', legend=False)\n",
    "    plt.title('SMOTE Upsampled Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/4-upsample/upsampled_plot_{csv_file.replace(\".csv\", \".png\")}')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ],
   "id": "aec5d143b9fcc429"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
