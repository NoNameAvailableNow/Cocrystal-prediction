{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV,\n",
    "                                   cross_validate, KFold, learning_curve)\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_curve, auc, precision_recall_curve, confusion_matrix,\n",
    "                             make_scorer)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import xgboost as xgb"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "classifiers = {\n",
    "    \"lr\": LogisticRegression(max_iter=2000),\n",
    "    \"svm\": SVC(probability=True),\n",
    "    \"rf\": RandomForestClassifier(random_state=42),\n",
    "    \"ann\": MLPClassifier(max_iter=2000, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "     \"lr\": {\n",
    "        \"C\": [0.001, 0.01, 0.1],\n",
    "        \"solver\": ['liblinear'],\n",
    "        \"penalty\": ['l1', 'l2']\n",
    "    },\n",
    "    \"svm\": {\n",
    "        \"C\": [0.01, 0.1],\n",
    "        \"kernel\": ['linear'],\n",
    "        \"gamma\": ['scale']\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"n_estimators\": [50, 100],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"min_samples_split\": [10, 20],\n",
    "        \"min_samples_leaf\": [5, 10],\n",
    "        \"max_features\": ['sqrt', 0.3]\n",
    "    },\n",
    "    \"ann\": {\n",
    "        \"hidden_layer_sizes\": [(10,), (20,)],\n",
    "        \"activation\": ['relu'],\n",
    "        \"alpha\": [0.1, 1.0],\n",
    "        \"learning_rate_init\": [0.001],\n",
    "        \"early_stopping\": [True]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'n_estimators': [100, 200],\n",
    "        'subsample': [0.7],\n",
    "        'colsample_bytree': [0.7],\n",
    "        'reg_alpha': [1.0, 10.0],\n",
    "        'reg_lambda': [1.0, 10.0],\n",
    "        'min_child_weight': [5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    \"no_fs\": 'data/4-upsample/upsampled_nofs_features.csv',\n",
    "    \"pca_fs\": 'data/4-upsample/upsampled_pca_features.csv',\n",
    "    \"rfe_fs\": 'data/4-upsample/upsampled_rfe_features.csv',\n",
    "    \"kbest_fs\": 'data/4-upsample/upsampled_kbest_features.csv'\n",
    "}\n",
    "\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    dataset_folder = f\"Results/Model_Results_{dataset_name}\"\n",
    "    for subfolder in [\"confusion_matrices\", \"roc_auc_curves\",\n",
    "                     \"precision_recall_curves\", \"learning_curves\"]:\n",
    "        os.makedirs(os.path.join(dataset_folder, subfolder), exist_ok=True)\n",
    "\n",
    "\n",
    "all_results = {\n",
    "    dataset_name: {'train_test': [], 'cv': []}\n",
    "    for dataset_name in datasets.keys()\n",
    "}\n",
    "\n",
    "fold = 5\n",
    "\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "scorers = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'specificity': make_scorer(specificity_score),\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'pr_auc': 'average_precision'\n",
    "}"
   ],
   "id": "84296e0fca88d326"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_train_test = []\n",
    "results_cv = []\n",
    "best_classifiers = {}\n",
    "\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "    X = df.drop(['label'], axis=1) if 'SMILES' in df.columns else df.drop(['label'], axis=1)\n",
    "    y = df['label']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    model_save_path = os.path.join(\"Predict\", dataset_name)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    roc_data = []\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        try:\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Training {name.upper()} model on {dataset_name} dataset...\")\n",
    "\n",
    "            if name == \"XGBoost\":\n",
    "                grid_search = GridSearchCV(clf, param_grids[name], cv=fold, scoring='accuracy', n_jobs=-1)\n",
    "                grid_search.fit(X_train_scaled, y_train,\n",
    "                             eval_set=[(X_test_scaled, y_test)],\n",
    "                             verbose=True),\n",
    "                best_clf = grid_search.best_estimator_\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "                print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\")\n",
    "            else:\n",
    "                grid_search = GridSearchCV(clf, param_grids[name], cv=fold, scoring='accuracy', n_jobs=-1)\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                best_clf = grid_search.best_estimator_\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "                print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "            training_start = time.perf_counter()\n",
    "            best_clf.fit(X_train_scaled, y_train)\n",
    "            training_end = time.perf_counter()\n",
    "            train_time = training_end - training_start\n",
    "\n",
    "            y_pred = best_clf.predict(X_test_scaled)\n",
    "            y_pred_proba = best_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            test_cm = confusion_matrix(y_test, y_pred)\n",
    "            tn, fp, fn, tp = test_cm.ravel()\n",
    "            spec = tn / (tn + fp)\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            roc_data.append((name, fpr, tpr, roc_auc))\n",
    "\n",
    "\n",
    "            all_results[dataset_name]['train_test'].append([\n",
    "                f\"{name}_{dataset_name}\",\n",
    "                f\"{train_time:.4f}\",\n",
    "                f\"{acc * 100:.2f}\",\n",
    "                f\"{prec * 100:.2f}\",\n",
    "                f\"{rec * 100:.2f}\",\n",
    "                f\"{f1 * 100:.2f}\",\n",
    "                f\"{spec * 100:.2f}\",\n",
    "                f\"{roc_auc * 100:.2f}\",\n",
    "                f\"{pr_auc * 100:.2f}\"\n",
    "            ])\n",
    "\n",
    "\n",
    "            cv_scores = cross_validate(best_clf, X_train_scaled, y_train,\n",
    "                                    cv=fold, scoring=scorers, return_train_score=False)\n",
    "\n",
    "\n",
    "            all_results[dataset_name]['cv'].append([\n",
    "                f\"{name}_{dataset_name}\",\n",
    "                f\"{train_time:.4f}\",\n",
    "                f\"{cv_scores['test_accuracy'].mean()*100:.2f} ± {cv_scores['test_accuracy'].std()*100:.2f}\",\n",
    "                f\"{cv_scores['test_precision'].mean()*100:.2f} ± {cv_scores['test_precision'].std()*100:.2f}\",\n",
    "                f\"{cv_scores['test_recall'].mean()*100:.2f} ± {cv_scores['test_recall'].std()*100:.2f}\",\n",
    "                f\"{cv_scores['test_f1'].mean()*100:.2f} ± {cv_scores['test_f1'].std()*100:.2f}\",\n",
    "                f\"{cv_scores['test_specificity'].mean()*100:.2f} ± {cv_scores['test_specificity'].std()*100:.2f}\",\n",
    "                f\"{cv_scores['test_roc_auc'].mean()*100:.2f} ± {cv_scores['test_roc_auc'].std()*100:.2f}\",\n",
    "                f\"{cv_scores['test_pr_auc'].mean()*100:.2f} ± {cv_scores['test_pr_auc'].std()*100:.2f}\"\n",
    "            ])\n",
    "\n",
    "            dataset_results_folder = f\"Results/Model_Results_{dataset_name}\"\n",
    "\n",
    "            plt.figure(figsize=(8, 6), dpi=600)\n",
    "            sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix - {name} ({dataset_name})')\n",
    "            plt.savefig(os.path.join(dataset_results_folder, \"confusion_matrices\", f'confusion_matrix_{name}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(8, 6), dpi=600)\n",
    "            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {name} ({dataset_name})')\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(dataset_results_folder, \"roc_auc_curves\", f'roc_curve_{name}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(8, 6), dpi=600)\n",
    "            plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title(f'Precision-Recall Curve - {name} ({dataset_name})')\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(dataset_results_folder, \"precision_recall_curves\", f'pr_curve_{name}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "            train_sizes, train_scores, test_scores = learning_curve(\n",
    "                best_clf, X_train_scaled, y_train,\n",
    "                train_sizes=train_sizes,\n",
    "                cv=fold,\n",
    "                n_jobs=-1,\n",
    "                scoring='accuracy'\n",
    "            )\n",
    "\n",
    "            train_mean = np.mean(train_scores, axis=1)\n",
    "            train_std = np.std(train_scores, axis=1)\n",
    "            test_mean = np.mean(test_scores, axis=1)\n",
    "            test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "            plt.figure(figsize=(8, 6), dpi=600)\n",
    "            plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training score')\n",
    "            plt.plot(train_sizes, test_mean, 'o-', color='g', label='Cross-validation score')\n",
    "\n",
    "            plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "            plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
    "\n",
    "            plt.xlabel('Training Examples')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title(f'Learning Curve - {name} ({dataset_name})')\n",
    "            plt.grid(True)\n",
    "            plt.legend(loc='best')\n",
    "\n",
    "            plt.savefig(os.path.join(dataset_results_folder, \"learning_curves\", f'learning_curve_{name}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            model_file = os.path.join(model_save_path, f'{name}_model.joblib')\n",
    "            joblib.dump(best_clf, model_file)\n",
    "\n",
    "            scaler_file = os.path.join(model_save_path, f'{name}_scaler.joblib')\n",
    "            joblib.dump(scaler, scaler_file)\n",
    "\n",
    "            params_file = os.path.join(model_save_path, f'{name}_best_params.txt')\n",
    "            with open(params_file, 'w') as f:\n",
    "                f.write(str(grid_search.best_params_))\n",
    "\n",
    "            print(f\"Successfully completed {name} on {dataset_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError occurred while processing {name} on {dataset_name}:\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    plt.figure(figsize=(8, 6), dpi=600)\n",
    "    for name, fpr, tpr, roc_auc in roc_data:\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - All Models ({dataset_name})')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    dataset_results_folder = f\"Results/Model_Results_{dataset_name}\"\n",
    "    plt.savefig(os.path.join(dataset_results_folder, \"roc_auc_curves\", 'roc_curve_all_models.png'))\n",
    "    plt.close()\n",
    "\n",
    "columns = ['Model', 'Time (s)', 'Accuracy (%)', 'Precision (%)', 'Recall (%)',\n",
    "          'F1-Score (%)', 'specificity (%)', 'ROC-AUC (%)', 'PR AUC (%)']\n",
    "\n",
    "\n",
    "all_train_test_results = []\n",
    "all_cv_results = []\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    all_train_test_results.extend(all_results[dataset_name]['train_test'])\n",
    "    all_cv_results.extend(all_results[dataset_name]['cv'])\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(all_train_test_results, columns=columns)\n",
    "df_cv = pd.DataFrame(all_cv_results, columns=columns)\n",
    "\n",
    "\n",
    "df_results.to_csv('Results/Train_Test_Results_All.csv', index=False)\n",
    "df_cv.to_csv('Results/Cross_Validation_Results_All.csv', index=False)\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ],
   "id": "4ba2b1b01df0a9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
